





# Bot check

# HW_ID: phds_hw2
# Бот проверит этот ID и предупредит, если случайно сдать что-то не то.

# Status: not final
# Перед отправкой в финальном решении удали "not" в строчке выше.
# Так бот проверит, что ты отправляешь финальную версию, а не промежуточную.
# Никакие значения в этой ячейке не влияют на факт сдачи работы.

# Profile: Biology





import numpy as np
import pandas as pd
from scipy.stats import norm

import matplotlib.pyplot as plt
import seaborn as sns

from tqdm import tqdm
from typing import Optional

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.base import BaseEstimator

import warnings
# warnings.filterwarnings("ignore")
import seaborn as sns
sns.set_theme(style="whitegrid")















































cancer = pd.read_csv("BRCA.csv")
cancer.head()





data = cancer.drop(["case_id", "submitter_id"], axis=1)
data.head()





data.info()


data.describe()





data_ready = data[['age_at_diagnosis', 'ESTIMATE', 'ABSOLUTE','IHC','CPE','LUMP']]








from sklearn.preprocessing import MinMaxScaler
data_ready = MinMaxScaler().fit_transform(data_ready)
data_ready


columns = ['age_at_diagnosis', 'ESTIMATE', 'ABSOLUTE','IHC','CPE','LUMP']
dataframe = pd.DataFrame(data_ready)
dataframe.columns = columns
cat_features = data_ready[:,0:5]
target = data_ready[:, 5]
dataframe.head()


X = cat_features
y = target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)


print(f"Тип данных IHC: {dataframe['IHC'].dtype}")
print(f"Уникальные значения IHC: {dataframe['IHC'].unique()}")
print(f"Количество уникальных значений: {dataframe['IHC'].nunique()}")
print(f"Все значения IHC:\n{dataframe['IHC'].value_counts().sort_index()}")











target_feature = dataframe['LUMP']
sns.regplot(x='age_at_diagnosis', y=target_feature, data = dataframe, line_kws={'color': 'red'}) 
plt.title("Зависимость LUMP от возраса опухоли")
plt.show()

sns.regplot(x='ESTIMATE', y = target_feature, data = dataframe, line_kws={'color': 'red'})
plt.title("Зависимость LUMP от ESTIMATE")
plt.show()

sns.regplot(x='ABSOLUTE', y = target_feature, data = dataframe, line_kws={'color': 'red'})
plt.title("Зависимость LUMP от ABSOLUTE")
plt.show

sns.regplot(x=dataframe['IHC'].astype(float), y = target_feature, data = dataframe, line_kws={'color': 'red'})
plt.title("Зависимость LUMP от IHC")
plt.show()

sns.regplot(x='CPE', y = target_feature, data = dataframe, line_kws={'color': 'red'})
plt.title("Зависимость LUMP от CPE")
plt.show()





mu = target.mean()
sigma = target.std()
sns.histplot(target, stat='density', bins=20, alpha=0.6)
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
y = norm.pdf(x, mu, sigma)
plt.plot(x, y, 'r-', linewidth=2, label=f'N({mu:.2f}, {sigma:.2f}²)')
plt.show()











sk_model = LinearRegression(fit_intercept=False)
sk_model.fit(X_train, y_train)









np.round(sk_model.coef_, 3)











<...>





<...>








<...>








<...>





<...>








def plot_linear_effects_grid(model: LinearRegression, X_train: pd.DataFrame, ncols: int = 4):
    """
    Сетка графиков для функций y = coef[i] * x_i + intercept
    (показывает наклон аппроксимационной прямой обученной линейной модели).

    Args:
        model (LinearRegression): обученная модель
        X_train (pd.DataFrame): обучающая выборка
        ncols (int): количество столбцов сетки графиков
    """
    names = <...>
    X = <...>

    coefs = <...>
    intercept = <...>

    n_feats = X.shape[1]
    nrows = int(np.ceil(n_feats / ncols))

    fig, axes = plt.subplots(nrows, ncols, figsize=(4.5*ncols, 3.5*nrows))
    axes = np.array(axes).ravel()

    for i, ax in enumerate(axes):
        if i >= n_feats:
            ax.axis("off")
            continue

        x = X[:, i]
        lo, hi = x.min(), x.max()   # границы сетки
        grid = np.linspace(lo, hi, 200)

        # линия модели
        <...>

        # точки (реальные значения признака → вклад в линейную часть)
        <...>

        # настройте внешний вид графика, если необходимо
        <...>

    plt.tight_layout()
    plt.show()















<...>





<...>


X_train, X_test, y_train, y_test = <...>

cat_features = <...>
num_features = <...>


onehotencoder = <...>

X_train_cat = <...>
X_test_cat  = <...>

X_train_num = <...>
X_test_num  = <...>





X_train_ohe = <...>
X_test_ohe = <...>





<...>





low_frec = <...>


X_train_cat, X_test_cat = <...>





<...>


<...>

















class MyLinearRegression(BaseEstimator):
    """
    Простейшая линейная регрессия с двумя способами обучения:
    - 'formula' — аналитическая форма по нормальным уравнениям (OLS);
    - 'gd'      — градиентный спуск по MSE.

    Модель: y = X * theta + epsilon, где X ∈ R^{n×d}, y ∈ R^{n×1}.
    """

    def __init__(self,
                 method: str = 'formula',
                 intercept: bool = True,
                 learning_rate: float = 0.01,
                 iter: int = 50) -> None:
        """
        Параметры
        ----------
        method : {'formula','gd'}, default='formula'
            Метод обучения.
        intercept : bool, default=True
            Добавлять ли свободный член (столбец единиц).
            ВАЖНО: интерсепт добавляется и в fit(), и в predict().
        learning_rate : float, default=0.01
            Скорость обучения для GD.
        iter : int, default=50
            Максимальное число итераций для GD.
        """
        self.method = method
        self.intercept = intercept
        self.learning_rate = learning_rate
        self.iter = iter

        # Атрибуты, заполняемые после fit()
        self.n = None
        self.d = None
        self.theta = None         # shape (d, 1)
        self.coef_ = None         # alias на theta без интерсепта (если есть)
        self.intercept_ = None    # alias на свободный член (если есть)

    def fit(self, X: np.ndarray, y) -> "MyLinearRegression":
        """
        Обучает модель на данных (X, y).

        Поведение:
        - Если intercept=True, к X добавляется последний столбец единиц.
        - Далее вызывается выбранный метод: 'formula' или 'gd'.

        Параметры
        ----------
        X : np.ndarray, shape (n, d0)
            Матрица признаков (БЕЗ интерсепта — он добавится здесь при необходимости).
        y : pandas.Series или np.ndarray
            Вектор/столбец отклика.

        Возвращает
        ----------
        self : MyLinearRegression
        """
        # добавляем интерсепт, если требуется
        if self.intercept:
            X = <...>

        if self.method == 'formula':
            self.__fit_formula(X, y)
        else:
            self.__fit_gd(X, y)

        # после обучения:
        theta_1d = self.theta.ravel()
        if self.intercept:
            self.intercept_ = float(theta_1d[-1])
            self.coef_ = theta_1d[:-1].copy()
        else:
            self.intercept_ = 0.0
            self.coef_ = theta_1d.copy()

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Возвращает предсказания для новых объектов X.

        ВАЖНО:
        - Если intercept=True, здесь также добавляется столбец единиц.
        - Ожидается, что .fit() уже был вызван и self.theta определена.

        Параметры
        ----------
        X : np.ndarray, shape (n, d0)
            Матрица новых объектов (без интерсепта, если intercept=True).

        Возвращает
        ----------
        y_pred : np.ndarray, shape (n, 1)
            Вектор предсказаний.
        """
        if self.theta is None:
            raise RuntimeError("Сначала вызовите .fit(X, y) — параметры модели ещё не оценены.")

        if self.intercept:
            X = <...>

        y_pred = <...>
        return <...>

    def __fit_formula(self, X: np.ndarray, y) -> "MyLinearRegression":
        """
        Обучение в замкнутой форме по нормальным уравнениям (OLS).

        Предполагается модель: y = X * theta + epsilon,
        где:
            X — матрица признаков (n × d),
            y — столбец отклика (n × 1) или вектор длины n.

        ВАЖНО:
        - Интерсепт (столбец единиц) сюда уже добавлен в fit().

        Возвращает
        ----------
        self : MyLinearRegression
        """
        self.n, self.d = X.shape[0], X.shape[1]
        # привести y к столбцу (n,1)
        y = <...>
        # явная инверсия:
        theta = <...>
        # гарантируем столбец (d,1)
        self.theta = <...>
        return self

    def __fit_gd(self, X: np.ndarray, y) -> "MyLinearRegression":
        """
        Обучение градиентным спуском для MSE.

        Параметры
        ----------
        X : np.ndarray, shape (n, d)
            Матрица признаков (интерсепт уже добавлен в fit(), если intercept=True).
        y : pandas.Series или np.ndarray формы (n,) или (n,1)
            Целевой вектор. В исходной логике используется y.values для Series.

        Возвращает
        ----------
        self : MyLinearRegression
        """
        self.n, self.d = X.shape[0], X.shape[1]

        # инициализация параметров случайными малыми значениями
        self.theta = <...>

        # приведение target к (n, 1) — точно как в твоей версии
        # поддержка и pandas.Series, и np.ndarray
        y = <...>

        # базовый цикл градиентного спуска
        for _ in tqdm(range(self.iter)):
            # градиент MSE по theta
            gradients = <...>
            # шаг GD
            self.theta = <...>

        return self






<...>





<...>











<...>





<...>








<...>


<...>








<...>









